{
  "block_size": 1024,
  "batch_size": 8,
  "warmup_epochs": 1,
  "cosine_epochs": 2,
  "learning_rate": 0.0003,
  "eval_every_steps": 8000,
  "vocab_size_limit": 30000,
  "merges_count": 5,
  "special_tokens": ["<|endoftext|>"],
  "tokenization_type": "byte" 
}

  