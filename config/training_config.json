{
  "block_size": 1024,
  "batch_size": 4,
  "warmup_epochs": 1,
  "cosine_epochs": 0,
  "learning_rate": 0.0003,
  "eval_every_steps": 3500,
  "vocab_size_limit": 30000,
  "merges_count": 2,
  "special_tokens": ["<|endoftext|>"],
  "tokenization_type": "huggingface", 
  "hf_tokenizer_name": "gpt2",
  "gpu_ids": [0, 1],
  "distributed": false
}

  